"""
íŒŒì¼ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
íŒŒì¼ ì½ê¸°, ì“°ê¸°, ì••ì¶• ë“± íŒŒì¼ ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

import os
import json
import csv
import zipfile
import gzip
import shutil
import hashlib
import pickle
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Generator
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)

class FileError(Exception):
    """íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜"""
    pass

class FileManager:
    """íŒŒì¼ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, base_path: Union[str, Path] = None):
        self.base_path = Path(base_path) if base_path else Path.cwd()
        self.base_path.mkdir(parents=True, exist_ok=True)
    
    def ensure_directory(self, path: Union[str, Path]) -> Path:
        """ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        dir_path = Path(path)
        if not dir_path.is_absolute():
            dir_path = self.base_path / dir_path
        
        dir_path.mkdir(parents=True, exist_ok=True)
        return dir_path
    
    def get_file_info(self, file_path: Union[str, Path]) -> Dict[str, Any]:
        """íŒŒì¼ ì •ë³´ ì¡°íšŒ"""
        path = Path(file_path)
        if not path.is_absolute():
            path = self.base_path / path
        
        if not path.exists():
            raise FileError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
        
        stat = path.stat()
        
        return {
            'name': path.name,
            'path': str(path),
            'size': stat.st_size,
            'size_mb': stat.st_size / (1024 * 1024),
            'created': datetime.fromtimestamp(stat.st_ctime),
            'modified': datetime.fromtimestamp(stat.st_mtime),
            'accessed': datetime.fromtimestamp(stat.st_atime),
            'extension': path.suffix,
            'is_file': path.is_file(),
            'is_dir': path.is_dir()
        }
    
    def list_files(self, directory: Union[str, Path], 
                   pattern: str = "*", recursive: bool = False) -> List[Path]:
        """ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        dir_path = Path(directory)
        if not dir_path.is_absolute():
            dir_path = self.base_path / dir_path
        
        if not dir_path.exists():
            raise FileError(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {dir_path}")
        
        if recursive:
            return list(dir_path.rglob(pattern))
        else:
            return list(dir_path.glob(pattern))
    
    def copy_file(self, source: Union[str, Path], 
                  destination: Union[str, Path], 
                  overwrite: bool = False) -> Path:
        """íŒŒì¼ ë³µì‚¬"""
        src_path = Path(source)
        dest_path = Path(destination)
        
        if not src_path.is_absolute():
            src_path = self.base_path / src_path
        if not dest_path.is_absolute():
            dest_path = self.base_path / dest_path
        
        if not src_path.exists():
            raise FileError(f"ì›ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {src_path}")
        
        if dest_path.exists() and not overwrite:
            raise FileError(f"ëŒ€ìƒ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {dest_path}")
        
        # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ìƒì„±
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        shutil.copy2(src_path, dest_path)
        logger.info(f"íŒŒì¼ ë³µì‚¬ ì™„ë£Œ: {src_path} -> {dest_path}")
        
        return dest_path
    
    def move_file(self, source: Union[str, Path], 
                  destination: Union[str, Path]) -> Path:
        """íŒŒì¼ ì´ë™"""
        src_path = Path(source)
        dest_path = Path(destination)
        
        if not src_path.is_absolute():
            src_path = self.base_path / src_path
        if not dest_path.is_absolute():
            dest_path = self.base_path / dest_path
        
        if not src_path.exists():
            raise FileError(f"ì›ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {src_path}")
        
        # ëŒ€ìƒ ë””ë ‰í† ë¦¬ ìƒì„±
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        shutil.move(str(src_path), str(dest_path))
        logger.info(f"íŒŒì¼ ì´ë™ ì™„ë£Œ: {src_path} -> {dest_path}")
        
        return dest_path
    
    def delete_file(self, file_path: Union[str, Path], 
                   confirm: bool = True) -> bool:
        """íŒŒì¼ ì‚­ì œ"""
        path = Path(file_path)
        if not path.is_absolute():
            path = self.base_path / path
        
        if not path.exists():
            logger.warning(f"ì‚­ì œí•  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
            return False
        
        if confirm:
            response = input(f"íŒŒì¼ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? {path} (y/N): ")
            if response.lower() != 'y':
                return False
        
        try:
            if path.is_file():
                path.unlink()
            elif path.is_dir():
                shutil.rmtree(path)
            
            logger.info(f"íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {path}")
            return True
        except Exception as e:
            logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_file_hash(self, file_path: Union[str, Path], 
                     algorithm: str = 'md5') -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        path = Path(file_path)
        if not path.is_absolute():
            path = self.base_path / path
        
        if not path.exists():
            raise FileError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
        
        hash_algo = getattr(hashlib, algorithm)()
        
        with open(path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_algo.update(chunk)
        
        return hash_algo.hexdigest()
    
    def cleanup_old_files(self, directory: Union[str, Path], 
                         days_old: int = 30, 
                         pattern: str = "*") -> List[Path]:
        """ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬"""
        dir_path = Path(directory)
        if not dir_path.is_absolute():
            dir_path = self.base_path / dir_path
        
        if not dir_path.exists():
            return []
        
        cutoff_date = datetime.now() - timedelta(days=days_old)
        deleted_files = []
        
        for file_path in dir_path.glob(pattern):
            if file_path.is_file():
                file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                if file_time < cutoff_date:
                    try:
                        file_path.unlink()
                        deleted_files.append(file_path)
                        logger.info(f"ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {file_path}")
                    except Exception as e:
                        logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        return deleted_files

class DataFileHandler:
    """ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def read_json(file_path: Union[str, Path], 
                  encoding: str = 'utf-8') -> Dict[str, Any]:
        """JSON íŒŒì¼ ì½ê¸°"""
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"JSON íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise FileError(f"JSON íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    @staticmethod
    def write_json(data: Dict[str, Any], file_path: Union[str, Path], 
                   encoding: str = 'utf-8', indent: int = 2) -> bool:
        """JSON íŒŒì¼ ì“°ê¸°"""
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'w', encoding=encoding) as f:
                json.dump(data, f, indent=indent, ensure_ascii=False)
            
            logger.info(f"JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            return True
        except Exception as e:
            logger.error(f"JSON íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    @staticmethod
    def read_csv(file_path: Union[str, Path], 
                 encoding: str = 'utf-8', **kwargs) -> pd.DataFrame:
        """CSV íŒŒì¼ ì½ê¸°"""
        try:
            return pd.read_csv(file_path, encoding=encoding, **kwargs)
        except Exception as e:
            logger.error(f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise FileError(f"CSV íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    @staticmethod
    def write_csv(df: pd.DataFrame, file_path: Union[str, Path], 
                  encoding: str = 'utf-8', index: bool = False, **kwargs) -> bool:
        """CSV íŒŒì¼ ì“°ê¸°"""
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            
            df.to_csv(file_path, encoding=encoding, index=index, **kwargs)
            logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            return True
        except Exception as e:
            logger.error(f"CSV íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    @staticmethod
    def read_excel(file_path: Union[str, Path], 
                   sheet_name: str = None, **kwargs) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
        """Excel íŒŒì¼ ì½ê¸°"""
        try:
            return pd.read_excel(file_path, sheet_name=sheet_name, **kwargs)
        except Exception as e:
            logger.error(f"Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise FileError(f"Excel íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    @staticmethod
    def write_excel(data: Union[pd.DataFrame, Dict[str, pd.DataFrame]], 
                    file_path: Union[str, Path], 
                    index: bool = False, **kwargs) -> bool:
        """Excel íŒŒì¼ ì“°ê¸°"""
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            
            if isinstance(data, pd.DataFrame):
                data.to_excel(file_path, index=index, **kwargs)
            elif isinstance(data, dict):
                with pd.ExcelWriter(file_path, **kwargs) as writer:
                    for sheet_name, df in data.items():
                        df.to_excel(writer, sheet_name=sheet_name, index=index)
            
            logger.info(f"Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            return True
        except Exception as e:
            logger.error(f"Excel íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}")
            return False
    
    @staticmethod
    def read_pickle(file_path: Union[str, Path]) -> Any:
        """Pickle íŒŒì¼ ì½ê¸°"""
        try:
            with open(file_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            logger.error(f"Pickle íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            raise FileError(f"Pickle íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    @staticmethod
    def write_pickle(data: Any, file_path: Union[str, Path]) -> bool:
        """Pickle íŒŒì¼ ì“°ê¸°"""
        try:
            # ë””ë ‰í† ë¦¬ ìƒì„±
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, 'wb') as f:
                pickle.dump(data, f)
            
            logger.info(f"Pickle íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            return True
        except Exception as e:
            logger.error(f"Pickle íŒŒì¼ ì“°ê¸° ì‹¤íŒ¨: {e}")
            return False

class CompressionHandler:
    """ì••ì¶• ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def compress_file(file_path: Union[str, Path], 
                     compression_type: str = 'gzip') -> Path:
        """íŒŒì¼ ì••ì¶•"""
        source_path = Path(file_path)
        
        if not source_path.exists():
            raise FileError(f"ì••ì¶•í•  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_path}")
        
        if compression_type == 'gzip':
            compressed_path = source_path.with_suffix(source_path.suffix + '.gz')
            
            with open(source_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
        
        elif compression_type == 'zip':
            compressed_path = source_path.with_suffix('.zip')
            
            with zipfile.ZipFile(compressed_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                zipf.write(source_path, source_path.name)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì••ì¶• í˜•ì‹: {compression_type}")
        
        logger.info(f"íŒŒì¼ ì••ì¶• ì™„ë£Œ: {source_path} -> {compressed_path}")
        return compressed_path
    
    @staticmethod
    def decompress_file(compressed_path: Union[str, Path], 
                       output_path: Union[str, Path] = None) -> Path:
        """íŒŒì¼ ì••ì¶• í•´ì œ"""
        source_path = Path(compressed_path)
        
        if not source_path.exists():
            raise FileError(f"ì••ì¶• íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_path}")
        
        if output_path is None:
            if source_path.suffix == '.gz':
                output_path = source_path.with_suffix('')
            elif source_path.suffix == '.zip':
                output_path = source_path.parent / source_path.stem
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì••ì¶• í˜•ì‹: {source_path.suffix}")
        
        output_path = Path(output_path)
        
        if source_path.suffix == '.gz':
            with gzip.open(source_path, 'rb') as f_in:
                with open(output_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
        
        elif source_path.suffix == '.zip':
            with zipfile.ZipFile(source_path, 'r') as zipf:
                zipf.extractall(output_path.parent)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì••ì¶• í˜•ì‹: {source_path.suffix}")
        
        logger.info(f"íŒŒì¼ ì••ì¶• í•´ì œ ì™„ë£Œ: {source_path} -> {output_path}")
        return output_path
    
    @staticmethod
    def create_archive(files: List[Union[str, Path]], 
                      archive_path: Union[str, Path], 
                      compression_type: str = 'zip') -> Path:
        """ì—¬ëŸ¬ íŒŒì¼ì„ í•˜ë‚˜ì˜ ì••ì¶• íŒŒì¼ë¡œ ìƒì„±"""
        archive_path = Path(archive_path)
        
        if compression_type == 'zip':
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file_path in files:
                    file_path = Path(file_path)
                    if file_path.exists():
                        zipf.write(file_path, file_path.name)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì••ì¶• í˜•ì‹: {compression_type}")
        
        logger.info(f"ì••ì¶• íŒŒì¼ ìƒì„± ì™„ë£Œ: {archive_path}")
        return archive_path

class BackupManager:
    """ë°±ì—… ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, backup_dir: Union[str, Path]):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def create_backup(self, source_path: Union[str, Path], 
                     backup_name: str = None) -> Path:
        """ë°±ì—… ìƒì„±"""
        source_path = Path(source_path)
        
        if not source_path.exists():
            raise FileError(f"ë°±ì—…í•  íŒŒì¼/ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_path}")
        
        if backup_name is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_name = f"{source_path.name}_{timestamp}"
        
        backup_path = self.backup_dir / backup_name
        
        if source_path.is_file():
            shutil.copy2(source_path, backup_path)
        elif source_path.is_dir():
            shutil.copytree(source_path, backup_path)
        
        logger.info(f"ë°±ì—… ìƒì„± ì™„ë£Œ: {source_path} -> {backup_path}")
        return backup_path
    
    def restore_backup(self, backup_name: str, 
                      restore_path: Union[str, Path]) -> Path:
        """ë°±ì—… ë³µì›"""
        backup_path = self.backup_dir / backup_name
        restore_path = Path(restore_path)
        
        if not backup_path.exists():
            raise FileError(f"ë°±ì—… íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {backup_path}")
        
        if restore_path.exists():
            response = input(f"ë³µì› ìœ„ì¹˜ì— ì´ë¯¸ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤. ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? {restore_path} (y/N): ")
            if response.lower() != 'y':
                return restore_path
        
        if backup_path.is_file():
            shutil.copy2(backup_path, restore_path)
        elif backup_path.is_dir():
            if restore_path.exists():
                shutil.rmtree(restore_path)
            shutil.copytree(backup_path, restore_path)
        
        logger.info(f"ë°±ì—… ë³µì› ì™„ë£Œ: {backup_path} -> {restore_path}")
        return restore_path
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """ë°±ì—… ëª©ë¡ ì¡°íšŒ"""
        backups = []
        
        for backup_path in self.backup_dir.iterdir():
            if backup_path.is_file() or backup_path.is_dir():
                stat = backup_path.stat()
                backups.append({
                    'name': backup_path.name,
                    'path': str(backup_path),
                    'size': stat.st_size,
                    'created': datetime.fromtimestamp(stat.st_ctime),
                    'modified': datetime.fromtimestamp(stat.st_mtime),
                    'type': 'file' if backup_path.is_file() else 'directory'
                })
        
        return sorted(backups, key=lambda x: x['created'], reverse=True)
    
    def cleanup_old_backups(self, days_old: int = 30) -> List[Path]:
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=days_old)
        deleted_backups = []
        
        for backup_path in self.backup_dir.iterdir():
            backup_time = datetime.fromtimestamp(backup_path.stat().st_ctime)
            if backup_time < cutoff_date:
                try:
                    if backup_path.is_file():
                        backup_path.unlink()
                    elif backup_path.is_dir():
                        shutil.rmtree(backup_path)
                    
                    deleted_backups.append(backup_path)
                    logger.info(f"ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {backup_path}")
                except Exception as e:
                    logger.error(f"ë°±ì—… ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        return deleted_backups

# ì „ì—­ íŒŒì¼ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
file_manager = FileManager()
data_handler = DataFileHandler()
compression_handler = CompressionHandler()

# í¸ì˜ í•¨ìˆ˜ë“¤
def read_json(file_path: Union[str, Path]) -> Dict[str, Any]:
    """JSON íŒŒì¼ ì½ê¸°"""
    return data_handler.read_json(file_path)

def write_json(data: Dict[str, Any], file_path: Union[str, Path]) -> bool:
    """JSON íŒŒì¼ ì“°ê¸°"""
    return data_handler.write_json(data, file_path)

def read_csv(file_path: Union[str, Path], **kwargs) -> pd.DataFrame:
    """CSV íŒŒì¼ ì½ê¸°"""
    return data_handler.read_csv(file_path, **kwargs)

def write_csv(df: pd.DataFrame, file_path: Union[str, Path], **kwargs) -> bool:
    """CSV íŒŒì¼ ì“°ê¸°"""
    return data_handler.write_csv(df, file_path, **kwargs)

def read_excel(file_path: Union[str, Path], **kwargs) -> pd.DataFrame:
    """Excel íŒŒì¼ ì½ê¸°"""
    return data_handler.read_excel(file_path, **kwargs)

def write_excel(data: pd.DataFrame, file_path: Union[str, Path], **kwargs) -> bool:
    """Excel íŒŒì¼ ì“°ê¸°"""
    return data_handler.write_excel(data, file_path, **kwargs)

def compress_file(file_path: Union[str, Path], compression_type: str = 'gzip') -> Path:
    """íŒŒì¼ ì••ì¶•"""
    return compression_handler.compress_file(file_path, compression_type)

def decompress_file(compressed_path: Union[str, Path], output_path: Union[str, Path] = None) -> Path:
    """íŒŒì¼ ì••ì¶• í•´ì œ"""
    return compression_handler.decompress_file(compressed_path, output_path)

def get_file_info(file_path: Union[str, Path]) -> Dict[str, Any]:
    """íŒŒì¼ ì •ë³´ ì¡°íšŒ"""
    return file_manager.get_file_info(file_path)

def list_files(directory: Union[str, Path], pattern: str = "*") -> List[Path]:
    """íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    return file_manager.list_files(directory, pattern)

def ensure_directory(path: Union[str, Path]) -> Path:
    """ë””ë ‰í† ë¦¬ ìƒì„±"""
    return file_manager.ensure_directory(path)

def get_file_hash(file_path: Union[str, Path], algorithm: str = 'md5') -> str:
    """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
    return file_manager.get_file_hash(file_path, algorithm)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    print("ğŸ“ íŒŒì¼ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
    test_dir = ensure_directory("test_files")
    print(f"í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±: {test_dir}")
    
    # JSON íŒŒì¼ í…ŒìŠ¤íŠ¸
    print("\nğŸ“„ JSON íŒŒì¼ í…ŒìŠ¤íŠ¸:")
    test_data = {
        "name": "í…ŒìŠ¤íŠ¸ ë°ì´í„°",
        "values": [1, 2, 3, 4, 5],
        "timestamp": datetime.now().isoformat()
    }
    
    json_file = test_dir / "test.json"
    success = write_json(test_data, json_file)
    print(f"JSON íŒŒì¼ ì €ì¥: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
    
    if success:
        loaded_data = read_json(json_file)
        print(f"JSON íŒŒì¼ ë¡œë“œ: {loaded_data['name']}")
    
    # CSV íŒŒì¼ í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š CSV íŒŒì¼ í…ŒìŠ¤íŠ¸:")
    test_df = pd.DataFrame({
        'A': [1, 2, 3, 4, 5],
        'B': [10, 20, 30, 40, 50],
        'C': ['ê°€', 'ë‚˜', 'ë‹¤', 'ë¼', 'ë§ˆ']
    })
    
    csv_file = test_dir / "test.csv"
    success = write_csv(test_df, csv_file)
    print(f"CSV íŒŒì¼ ì €ì¥: {'ì„±ê³µ' if success else 'ì‹¤íŒ¨'}")
    
    if success:
        loaded_df = read_csv(csv_file)
        print(f"CSV íŒŒì¼ ë¡œë“œ: {len(loaded_df)} í–‰")
    
    # íŒŒì¼ ì •ë³´ ì¡°íšŒ
    print("\nğŸ“‹ íŒŒì¼ ì •ë³´ ì¡°íšŒ:")
    if json_file.exists():
        file_info = get_file_info(json_file)
        print(f"íŒŒì¼ëª…: {file_info['name']}")
        print(f"í¬ê¸°: {file_info['size']} bytes")
        print(f"ìˆ˜ì •ì¼: {file_info['modified']}")
    
    # íŒŒì¼ ëª©ë¡ ì¡°íšŒ
    print("\nğŸ“‚ íŒŒì¼ ëª©ë¡ ì¡°íšŒ:")
    files = list_files(test_dir)
    for file_path in files:
        print(f"  {file_path.name}")
    
    # íŒŒì¼ í•´ì‹œ ê³„ì‚°
    print("\nğŸ” íŒŒì¼ í•´ì‹œ ê³„ì‚°:")
    if json_file.exists():
        file_hash = get_file_hash(json_file)
        print(f"MD5 í•´ì‹œ: {file_hash}")
    
    # ì••ì¶• í…ŒìŠ¤íŠ¸
    print("\nğŸ“¦ ì••ì¶• í…ŒìŠ¤íŠ¸:")
    if json_file.exists():
        try:
            compressed_file = compress_file(json_file, 'gzip')
            print(f"ì••ì¶• íŒŒì¼ ìƒì„±: {compressed_file}")
            
            decompressed_file = decompress_file(compressed_file)
            print(f"ì••ì¶• í•´ì œ: {decompressed_file}")
        except Exception as e:
            print(f"ì••ì¶• í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ë°±ì—… í…ŒìŠ¤íŠ¸
    print("\nğŸ’¾ ë°±ì—… í…ŒìŠ¤íŠ¸:")
    backup_mgr = BackupManager(test_dir / "backups")
    
    if json_file.exists():
        try:
            backup_path = backup_mgr.create_backup(json_file)
            print(f"ë°±ì—… ìƒì„±: {backup_path}")
            
            backups = backup_mgr.list_backups()
            print(f"ë°±ì—… ëª©ë¡: {len(backups)}ê°œ")
        except Exception as e:
            print(f"ë°±ì—… í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ì •ë¦¬
    print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬:")
    try:
        if test_dir.exists():
            shutil.rmtree(test_dir)
            print("í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì‚­ì œ ì™„ë£Œ")
    except Exception as e:
        print(f"ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    print("\nâœ… ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")