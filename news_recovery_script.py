#!/usr/bin/env python3
"""
ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ë°ì´í„° ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
7ì›” 11ì¼ ì´í›„ ëˆ„ë½ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë³µêµ¬

ì‹¤í–‰ ë°©ë²•:
python news_recovery_script.py --days=10 --batch_size=50
python news_recovery_script.py --quick_recovery --top_stocks=100
python news_recovery_script.py --full_recovery --start_date=2025-07-11
"""

import sys
import os
import argparse
import sqlite3
import requests
import json
import re
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import logging
import time
from urllib.parse import quote
from dotenv import load_dotenv
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class NewsRecoveryManager:
    """ë‰´ìŠ¤ ë°ì´í„° ë³µêµ¬ ë§¤ë‹ˆì €"""
    
    def __init__(self, log_level='INFO'):
        # ë¡œê¹… ì„¤ì •
        self.setup_logging(log_level)
        
        # API ì„¤ì •
        self.naver_client_id = os.getenv('NAVER_CLIENT_ID')
        self.naver_client_secret = os.getenv('NAVER_CLIENT_SECRET')
        
        if not self.naver_client_id or not self.naver_client_secret:
            raise ValueError("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        
        # ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
        self.stock_db_path = Path('data/databases/stock_data.db')
        self.news_db_path = Path('data/databases/news_data.db')
        self.dart_db_path = Path('data/databases/dart_data.db')
        
        # í†µê³„
        self.stats = {
            'total_companies': 0,
            'processed_companies': 0,
            'successful_companies': 0,
            'total_news_collected': 0,
            'failed_companies': [],
            'start_time': datetime.now(),
            'target_start_date': None
        }
        
        # ê¸ˆìœµ í‚¤ì›Œë“œ ì‚¬ì „ (í™•ì¥ë¨)
        self.financial_keywords = [
            # ì‹¤ì  ê´€ë ¨
            'ì‹¤ì ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ', 'ë°°ë‹¹', 'ë°°ë‹¹ê¸ˆ', 'ì£¼ë‹¹ìˆœì´ìµ', 'EPS', 'ROE', 'ROA',
            # íˆ¬ì ê´€ë ¨  
            'íˆ¬ì', 'ì¸ìˆ˜í•©ë³‘', 'M&A', 'ì§€ë¶„', 'íˆ¬ìê¸ˆ', 'í€ë”©', 'íˆ¬ììœ ì¹˜', 'ê¸°ì—…ê³µê°œ', 'IPO',
            # ì‚¬ì—… ê´€ë ¨
            'ì‹ ì œí’ˆ', 'ì¶œì‹œ', 'ê³„ì•½', 'ìˆ˜ì£¼', 'íŠ¹í—ˆ', 'ê¸°ìˆ ê°œë°œ', 'ì—°êµ¬ê°œë°œ', 'R&D', 'í˜ì‹ ',
            # ì‹œì„¤ ê´€ë ¨
            'ì¦ì„¤', 'ê³µì¥', 'ì‹œì„¤', 'ìƒì‚°', 'ê³µê¸‰', 'ê±´ì„¤', 'í™•ì¥', 'íˆ¬ìê³„íš',
            # ì£¼ê°€ ê´€ë ¨
            'ì£¼ê°€', 'ìƒìŠ¹', 'í•˜ë½', 'ëª©í‘œê°€', 'íˆ¬ìì˜ê²¬', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ë³´ìœ ', 'ì¤‘ë¦½',
            # ê¸°ì—… ê´€ë ¨
            'ë¶„í• ', 'í•©ë³‘', 'ìœ ìƒì¦ì', 'ë¬´ìƒì¦ì', 'ìì‚¬ì£¼', 'ì£¼ì‹', 'ìƒì¥', 'ìƒí',
            # ì¸ì‚¬ ê´€ë ¨
            'CEO', 'ëŒ€í‘œì´ì‚¬', 'ì„ì›', 'ì¸ì‚¬', 'ì¡°ì§ê°œí¸', 'ì‚¬ì¥', 'ë¶€ì‚¬ì¥',
            # ì¬ë¬´ ê´€ë ¨
            'ìê¸ˆ', 'ëŒ€ì¶œ', 'ì°¨ì…', 'ë¶€ì±„', 'í˜„ê¸ˆ', 'ìœ ë™ì„±', 'ì¬ë¬´êµ¬ì¡°',
            # ì‹œì¥ ê´€ë ¨
            'ì‹œì¥ì ìœ ìœ¨', 'ê²½ìŸë ¥', 'ì—…ê³„', 'ì‚°ì—…', 'ì„¹í„°', 'ë™í–¥'
        ]
        
        # ê°ì • ë¶„ì„ í‚¤ì›Œë“œ (í™•ì¥ë¨)
        self.positive_words = [
            'ì„±ì¥', 'ì¦ê°€', 'ìƒìŠ¹', 'ê°œì„ ', 'í™•ëŒ€', 'í˜¸ì¡°', 'ì¢‹ì€', 'ê¸ì •', 'ì„±ê³µ', 'ìš°ìˆ˜',
            'ë‹¬ì„±', 'ëŒíŒŒ', 'ìµœê³ ', 'ê°•ì„¸', 'ê¸°ëŒ€', 'ì „ë§', 'í˜ì‹ ', 'ë›°ì–´ë‚œ', 'íƒì›”í•œ',
            'í˜¸í™©', 'ê¸‰ë“±', 'ì‹ ê¸°ë¡', 'ìµœëŒ€', 'í™•ì¥', 'ì§„ì¶œ', 'ì„±ê³¼', 'ìˆ˜ìµ', 'ì´ìµ'
        ]
        
        self.negative_words = [
            'ê°ì†Œ', 'í•˜ë½', 'ë¶€ì§„', 'ì•…í™”', 'ì¶•ì†Œ', 'ìš°ë ¤', 'ë‚˜ìœ', 'ë¶€ì •', 'ì‹¤íŒ¨', 'ì†ì‹¤',
            'ë¶€ì¡±', 'ì ì', 'ìµœì €', 'ë¶€ì‹¤', 'ë¶ˆì•ˆ', 'ìœ„í—˜', 'í•˜í–¥', 'ì¹¨ì²´', 'íƒ€ê²©',
            'ê¸‰ë½', 'í­ë½', 'ìµœì•…', 'ìœ„ê¸°', 'ë¬¸ì œ', 'ì–´ë ¤ì›€', 'ë¶€ì±„', 'ì ì', 'ì†í•´'
        ]
    
    def setup_logging(self, log_level):
        """ë¡œê¹… ì„¤ì •"""
        # ë¡œê·¸ íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f'news_recovery_{timestamp}.log'
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=getattr(logging, log_level),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_filename, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ë‰´ìŠ¤ ë³µêµ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” - ë¡œê·¸íŒŒì¼: {log_filename}")
    
    def filter_news_by_date(self, news_items, start_date_str):
        """ë‚ ì§œë³„ ë‰´ìŠ¤ í•„í„°ë§"""
        try:
            start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
            filtered_items = []
            
            for item in news_items:
                pub_date_str = item.get('pubDate', '')
                if pub_date_str:
                    try:
                        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë‚ ì§œ í˜•ì‹: "Mon, 15 Jul 2025 14:30:00 +0900"
                        pub_date = datetime.strptime(pub_date_str, '%a, %d %b %Y %H:%M:%S %z')
                        pub_date_naive = pub_date.replace(tzinfo=None)  # timezone ì œê±°
                        
                        if pub_date_naive >= start_date:
                            filtered_items.append(item)
                    except ValueError:
                        # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ì‹œ í¬í•¨ (ì•ˆì „ì¥ì¹˜)
                        filtered_items.append(item)
                else:
                    # ë‚ ì§œ ì •ë³´ ì—†ìœ¼ë©´ í¬í•¨
                    filtered_items.append(item)
            
            return filtered_items
        except Exception as e:
            self.logger.error(f"ë‚ ì§œ í•„í„°ë§ ì‹¤íŒ¨: {e}")
            return news_items  # ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜

    def get_company_list(self, top_n=None, min_market_cap=None):
        """íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        try:
            companies = []
            
            # 1. stock_data.dbì—ì„œ íšŒì‚¬ ì •ë³´ ì¡°íšŒ
            if self.stock_db_path.exists():
                with sqlite3.connect(self.stock_db_path) as conn:
                    query = """
                        SELECT stock_code, company_name, market_cap, market_type
                        FROM company_info 
                        WHERE company_name IS NOT NULL AND company_name != ''
                    """
                    
                    if min_market_cap:
                        query += f" AND market_cap >= {min_market_cap}"
                    
                    query += " ORDER BY market_cap DESC"
                    
                    if top_n:
                        query += f" LIMIT {top_n}"
                    
                    stock_companies = pd.read_sql(query, conn)
                    companies.extend(stock_companies.to_dict('records'))
            
            # 2. dart_data.dbì—ì„œ ì¶”ê°€ íšŒì‚¬ ì •ë³´ ì¡°íšŒ (stock DBì— ì—†ëŠ” íšŒì‚¬ë“¤)
            if self.dart_db_path.exists():
                with sqlite3.connect(self.dart_db_path) as conn:
                    existing_codes = [c['stock_code'] for c in companies]
                    if existing_codes:
                        placeholder = ','.join(['?' for _ in existing_codes])
                        query = f"""
                            SELECT stock_code, corp_name as company_name, NULL as market_cap, 'DART' as market_type
                            FROM corp_codes 
                            WHERE stock_code IS NOT NULL AND stock_code != ''
                            AND stock_code NOT IN ({placeholder})
                        """
                        dart_companies = pd.read_sql(query, conn, params=existing_codes)
                    else:
                        query = """
                            SELECT stock_code, corp_name as company_name, NULL as market_cap, 'DART' as market_type
                            FROM corp_codes 
                            WHERE stock_code IS NOT NULL AND stock_code != ''
                        """
                        dart_companies = pd.read_sql(query, conn)
                    
                    companies.extend(dart_companies.to_dict('records'))
            
            # ì¤‘ë³µ ì œê±° (stock_code ê¸°ì¤€)
            seen_codes = set()
            unique_companies = []
            for company in companies:
                if company['stock_code'] not in seen_codes:
                    seen_codes.add(company['stock_code'])
                    unique_companies.append(company)
            
            self.logger.info(f"ğŸ“Š íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì™„ë£Œ: {len(unique_companies)}ê°œ íšŒì‚¬")
            return unique_companies
            
        except Exception as e:
            self.logger.error(f"íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def search_news_with_retry(self, keyword, start_date=None, max_retries=3, delay=1):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë‰´ìŠ¤ ê²€ìƒ‰"""
        for attempt in range(max_retries):
            try:
                headers = {
                    'X-Naver-Client-Id': self.naver_client_id,
                    'X-Naver-Client-Secret': self.naver_client_secret,
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                # ë‚ ì§œ í•„í„°ë§ì„ ìœ„í•œ ì¿¼ë¦¬ ìˆ˜ì •
                search_query = keyword
                if start_date:
                    # ë„¤ì´ë²„ ë‰´ìŠ¤ APIëŠ” ë‚ ì§œ ë²”ìœ„ ê²€ìƒ‰ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
                    # í›„ì²˜ë¦¬ì—ì„œ í•„í„°ë§í•˜ë„ë¡ í•¨
                    pass
                
                params = {
                    'query': search_query,
                    'display': 100,  # ìµœëŒ€ 100ê°œ
                    'start': 1,
                    'sort': 'date'  # ìµœì‹ ìˆœ
                }
                
                response = requests.get(
                    self.base_url, 
                    headers=headers, 
                    params=params, 
                    timeout=30
                )
                response.raise_for_status()
                
                data = response.json()
                news_items = data.get('items', [])
                
                # ë‚ ì§œ í•„í„°ë§ (start_date ì´í›„ë§Œ)
                if start_date:
                    news_items = self.filter_news_by_date(news_items, start_date)
                
                self.logger.debug(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì„±ê³µ (í‚¤ì›Œë“œ: {keyword}): {len(news_items)}ê±´")
                return news_items
                
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨ (ì‹œë„ {attempt+1}/{max_retries}, í‚¤ì›Œë“œ: {keyword}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(delay * (attempt + 1))  # ì§€ìˆ˜ì  ë°±ì˜¤í”„
                else:
                    self.logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ìµœì¢… ì‹¤íŒ¨ (í‚¤ì›Œë“œ: {keyword})")
                    return []
            except Exception as e:
                self.logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜ˆì™¸ (í‚¤ì›Œë“œ: {keyword}): {e}")
                return []
        
        return []
    
    def filter_financial_news(self, news_items, company_name=None):
        """ê¸ˆìœµ/í€ë”ë©˜í„¸ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§ (ê°œì„ ë¨)"""
        filtered_news = []
        
        for item in news_items:
            title = re.sub(r'<[^>]+>', '', item.get('title', '')).lower()
            description = re.sub(r'<[^>]+>', '', item.get('description', '')).lower()
            content = f"{title} {description}"
            
            # ì ìˆ˜ ê³„ì‚°
            relevance_score = 0
            
            # 1. ê¸ˆìœµ í‚¤ì›Œë“œ ì ìˆ˜
            financial_matches = sum(1 for keyword in self.financial_keywords if keyword.lower() in content)
            relevance_score += financial_matches * 2
            
            # 2. íšŒì‚¬ëª… ì ìˆ˜
            if company_name:
                company_matches = content.count(company_name.lower())
                relevance_score += company_matches * 3
            
            # 3. ì œëª©ì— íšŒì‚¬ëª…ì´ í¬í•¨ëœ ê²½ìš° ë³´ë„ˆìŠ¤
            if company_name and company_name.lower() in title:
                relevance_score += 5
            
            # 4. ìµœì†Œ ì ìˆ˜ ê¸°ì¤€ (ë” ì—„ê²©í•˜ê²Œ)
            if relevance_score >= 3:  # ìµœì†Œ 3ì  ì´ìƒ
                item['relevance_score'] = relevance_score
                filtered_news.append(item)
        
        self.logger.debug(f"ê¸ˆìœµ ë‰´ìŠ¤ í•„í„°ë§: {len(filtered_news)}/{len(news_items)}ê±´ ì„ íƒ")
        return filtered_news
    
    def analyze_sentiment(self, text):
        """ê°œì„ ëœ ê°ì •ë¶„ì„"""
        try:
            clean_text = re.sub(r'<[^>]+>', '', text).lower()
            
            # ê¸ì •/ë¶€ì • ë‹¨ì–´ ì¹´ìš´íŠ¸
            positive_count = sum(1 for word in self.positive_words if word in clean_text)
            negative_count = sum(1 for word in self.negative_words if word in clean_text)
            
            # ê°ì • ì ìˆ˜ ê³„ì‚°
            total_sentiment_words = positive_count + negative_count
            if total_sentiment_words == 0:
                sentiment_score = 0.0
                confidence = 0.1
            else:
                sentiment_score = (positive_count - negative_count) / total_sentiment_words
                confidence = min(total_sentiment_words / 5, 1.0)
            
            # ê°ì • ë¼ë²¨ ê²°ì •
            if sentiment_score > 0.2:
                sentiment_label = 'positive'
            elif sentiment_score < -0.2:
                sentiment_label = 'negative'
            else:
                sentiment_label = 'neutral'
            
            return {
                'sentiment_score': sentiment_score,
                'sentiment_label': sentiment_label,
                'confidence': confidence,
                'positive_count': positive_count,
                'negative_count': negative_count,
                'keywords': f"ê¸ì •:{positive_count},ë¶€ì •:{negative_count}"
            }
            
        except Exception as e:
            self.logger.error(f"ê°ì •ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'sentiment_score': 0.0,
                'sentiment_label': 'neutral',
                'confidence': 0.0,
                'positive_count': 0,
                'negative_count': 0,
                'keywords': ''
            }
    
    def save_news_to_database(self, news_data):
        """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
            self.news_db_path.parent.mkdir(parents=True, exist_ok=True)
            
            with sqlite3.connect(self.news_db_path) as conn:
                # í…Œì´ë¸” ìƒì„± (ì—†ëŠ” ê²½ìš°)
                self.create_news_tables(conn)
                
                saved_count = 0
                for news_item in news_data:
                    try:
                        # ì¤‘ë³µ í™•ì¸ (title + stock_code ê¸°ì¤€)
                        existing = conn.execute("""
                            SELECT COUNT(*) FROM news_articles 
                            WHERE title = ? AND stock_code = ?
                        """, (news_item['title'], news_item['stock_code'])).fetchone()[0]
                        
                        if existing == 0:
                            conn.execute('''
                                INSERT INTO news_articles 
                                (stock_code, title, description, originallink, link, pubDate, 
                                 source, category, sentiment_score, sentiment_label, confidence_score, keywords, created_at)
                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                            ''', (
                                news_item['stock_code'],
                                news_item['title'],
                                news_item['description'],
                                news_item['originallink'],
                                news_item['link'],
                                news_item['pubDate'],
                                news_item['source'],
                                news_item['category'],
                                news_item['sentiment_score'],
                                news_item['sentiment_label'],
                                news_item['confidence_score'],
                                news_item['keywords'],
                                datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                            ))
                            saved_count += 1
                    except sqlite3.Error as e:
                        self.logger.warning(f"ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                        continue
                
                conn.commit()
                return saved_count
                
        except Exception as e:
            self.logger.error(f"ë‰´ìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0
    
    def create_news_tables(self, conn):
        """ë‰´ìŠ¤ í…Œì´ë¸” ìƒì„±"""
        conn.execute('''
            CREATE TABLE IF NOT EXISTS news_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                stock_code TEXT NOT NULL,
                title TEXT NOT NULL,
                description TEXT,
                originallink TEXT,
                link TEXT,
                pubDate TEXT,
                source TEXT DEFAULT 'ë„¤ì´ë²„ë‰´ìŠ¤',
                category TEXT DEFAULT 'ê¸ˆìœµ',
                sentiment_score REAL DEFAULT 0.0,
                sentiment_label TEXT DEFAULT 'neutral',
                confidence_score REAL DEFAULT 0.0,
                keywords TEXT,
                created_at TEXT,
                UNIQUE(title, stock_code)
            )
        ''')
        
        conn.execute('''
            CREATE INDEX IF NOT EXISTS idx_news_stock_code ON news_articles(stock_code)
        ''')
        
        conn.execute('''
            CREATE INDEX IF NOT EXISTS idx_news_created_at ON news_articles(created_at)
        ''')
    
    def collect_news_for_company(self, company_info, start_date=None):
        """ë‹¨ì¼ íšŒì‚¬ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        stock_code = company_info['stock_code']
        company_name = company_info['company_name']
        
        try:
            self.logger.debug(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {company_name}({stock_code})")
            
            # íšŒì‚¬ëª…ìœ¼ë¡œ ë‰´ìŠ¤ ê²€ìƒ‰
            news_items = self.search_news_with_retry(company_name, start_date)
            
            if not news_items:
                self.logger.warning(f"ë‰´ìŠ¤ ì—†ìŒ: {company_name}")
                return 0
            
            # ê¸ˆìœµ ë‰´ìŠ¤ í•„í„°ë§
            filtered_news = self.filter_financial_news(news_items, company_name)
            
            if not filtered_news:
                self.logger.warning(f"ê¸ˆìœµ ë‰´ìŠ¤ ì—†ìŒ: {company_name}")
                return 0
            
            # ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬
            processed_news = []
            for item in filtered_news:
                # HTML íƒœê·¸ ì œê±°
                title = re.sub(r'<[^>]+>', '', item.get('title', ''))
                description = re.sub(r'<[^>]+>', '', item.get('description', ''))
                
                # ê°ì •ë¶„ì„
                sentiment = self.analyze_sentiment(f"{title} {description}")
                
                news_data = {
                    'stock_code': stock_code,
                    'title': title,
                    'description': description,
                    'originallink': item.get('originallink', ''),
                    'link': item.get('link', ''),
                    'pubDate': item.get('pubDate', ''),
                    'source': 'ë„¤ì´ë²„ë‰´ìŠ¤',
                    'category': 'ê¸ˆìœµ',
                    'sentiment_score': sentiment['sentiment_score'],
                    'sentiment_label': sentiment['sentiment_label'],
                    'confidence_score': sentiment['confidence'],
                    'keywords': sentiment['keywords']
                }
                processed_news.append(news_data)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            saved_count = self.save_news_to_database(processed_news)
            
            self.logger.info(f"âœ… {company_name}({stock_code}): {saved_count}ê±´ ì €ì¥")
            return saved_count
            
        except Exception as e:
            self.logger.error(f"âŒ {company_name}({stock_code}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.stats['failed_companies'].append({
                'stock_code': stock_code,
                'company_name': company_name,
                'error': str(e)
            })
            return 0
    
    def parallel_news_collection(self, companies, start_date=None, max_workers=5, delay_between_requests=0.2):
        """ë³‘ë ¬ ë‰´ìŠ¤ ìˆ˜ì§‘ (API ì œí•œ ê³ ë ¤)"""
        self.logger.info(f"ğŸš€ ë³‘ë ¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {len(companies)}ê°œ íšŒì‚¬, {max_workers}ê°œ ì›Œì»¤")
        
        total_news = 0
        successful_companies = 0
        
        # ThreadPoolExecutor ì‚¬ìš©í•˜ë˜, API ì œí•œì„ ìœ„í•´ ì ì€ ìˆ˜ì˜ ì›Œì»¤ ì‚¬ìš©
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ë¯¸ë˜ ê°ì²´ ìƒì„±
            future_to_company = {
                executor.submit(self.collect_news_for_company, company, start_date): company 
                for company in companies
            }
            
            for i, future in enumerate(as_completed(future_to_company)):
                company = future_to_company[future]
                
                try:
                    news_count = future.result()
                    if news_count > 0:
                        successful_companies += 1
                        total_news += news_count
                    
                    self.stats['processed_companies'] += 1
                    
                    # ì§„í–‰ë¥  í‘œì‹œ
                    if (i + 1) % 10 == 0:
                        progress = (i + 1) / len(companies) * 100
                        self.logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {i+1}/{len(companies)} ({progress:.1f}%) - ì„±ê³µ: {successful_companies}, ë‰´ìŠ¤: {total_news:,}ê±´")
                    
                    # API ì œí•œ ëŒ€ì‘
                    time.sleep(delay_between_requests)
                    
                except Exception as e:
                    self.logger.error(f"ë³‘ë ¬ ì²˜ë¦¬ ì˜¤ë¥˜ ({company['company_name']}): {e}")
                    continue
        
        self.stats['successful_companies'] = successful_companies
        self.stats['total_news_collected'] = total_news
        
        return total_news, successful_companies
    
    def sequential_news_collection(self, companies, start_date=None, delay_between_requests=0.3):
        """ìˆœì°¨ì  ë‰´ìŠ¤ ìˆ˜ì§‘ (ì•ˆì •ì )"""
        self.logger.info(f"ğŸŒ ìˆœì°¨ì  ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {len(companies)}ê°œ íšŒì‚¬")
        
        total_news = 0
        successful_companies = 0
        
        for i, company in enumerate(companies):
            try:
                news_count = self.collect_news_for_company(company, start_date)
                if news_count > 0:
                    successful_companies += 1
                    total_news += news_count
                
                self.stats['processed_companies'] += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 10 == 0:
                    progress = (i + 1) / len(companies) * 100
                    elapsed = datetime.now() - self.stats['start_time']
                    self.logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {i+1}/{len(companies)} ({progress:.1f}%) - ì„±ê³µ: {successful_companies}, ë‰´ìŠ¤: {total_news:,}ê±´, ê²½ê³¼: {elapsed}")
                
                # API ì œí•œ ëŒ€ì‘
                time.sleep(delay_between_requests)
                
            except Exception as e:
                self.logger.error(f"ìˆœì°¨ ì²˜ë¦¬ ì˜¤ë¥˜ ({company['company_name']}): {e}")
                continue
        
        self.stats['successful_companies'] = successful_companies
        self.stats['total_news_collected'] = total_news
        
        return total_news, successful_companies
    
    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        elapsed = datetime.now() - self.stats['start_time']
        
        self.logger.info("="*60)
        self.logger.info("ğŸ“Š ë‰´ìŠ¤ ë°ì´í„° ë³µêµ¬ ì™„ë£Œ - ìµœì¢… í†µê³„")
        self.logger.info("="*60)
        self.logger.info(f"ğŸ¢ ì „ì²´ íšŒì‚¬ ìˆ˜: {self.stats['total_companies']:,}ê°œ")
        self.logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {self.stats['processed_companies']:,}ê°œ")
        self.logger.info(f"ğŸ¯ ì„±ê³µí•œ íšŒì‚¬: {self.stats['successful_companies']:,}ê°œ")
        self.logger.info(f"ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {self.stats['total_news_collected']:,}ê±´")
        self.logger.info(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {elapsed}")
        
        if self.stats['target_start_date']:
            self.logger.info(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ ê¸°ê°„: {self.stats['target_start_date']} ì´í›„")
        
        if self.stats['failed_companies']:
            self.logger.info(f"âŒ ì‹¤íŒ¨í•œ íšŒì‚¬: {len(self.stats['failed_companies'])}ê°œ")
            for failed in self.stats['failed_companies'][:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                self.logger.info(f"   - {failed['company_name']}({failed['stock_code']}): {failed['error']}")
        
        # ì„±ê³µë¥  ê³„ì‚°
        if self.stats['processed_companies'] > 0:
            success_rate = (self.stats['successful_companies'] / self.stats['processed_companies']) * 100
            self.logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {success_rate:.1f}%")
        
        self.logger.info("="*60)
    
    def quick_recovery(self, top_stocks=100, start_date=None, parallel=True):
        """ë¹ ë¥¸ ë³µêµ¬ (ì£¼ìš” ì¢…ëª©ë§Œ)"""
        self.logger.info(f"âš¡ ë¹ ë¥¸ ë³µêµ¬ ì‹œì‘ (ìƒìœ„ {top_stocks}ê°œ ì¢…ëª©)")
        
        if start_date:
            self.logger.info(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ: {start_date} ì´í›„ ë‰´ìŠ¤")
            self.stats['target_start_date'] = start_date
        
        # ì‹œê°€ì´ì•¡ ìƒìœ„ ì¢…ëª©ë“¤ ì¡°íšŒ
        companies = self.get_company_list(top_n=top_stocks, min_market_cap=1000000000)  # 10ì–µ ì´ìƒ
        
        if not companies:
            self.logger.error("íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        self.stats['total_companies'] = len(companies)
        
        if parallel:
            total_news, successful = self.parallel_news_collection(companies, start_date, max_workers=3)
        else:
            total_news, successful = self.sequential_news_collection(companies, start_date)
        
        self.print_final_stats()
        return total_news > 0
    
    def full_recovery(self, start_date=None, parallel=False, batch_size=100):
        """ì „ì²´ ë³µêµ¬ (ëª¨ë“  ì¢…ëª©)"""
        self.logger.info("ğŸ­ ì „ì²´ ë³µêµ¬ ì‹œì‘ (ëª¨ë“  ì¢…ëª©)")
        
        if start_date:
            self.logger.info(f"ğŸ“… ìˆ˜ì§‘ ëŒ€ìƒ: {start_date} ì´í›„ ë‰´ìŠ¤")
            self.stats['target_start_date'] = start_date
        
        # ì „ì²´ ì¢…ëª© ì¡°íšŒ
        companies = self.get_company_list()
        
        if not companies:
            self.logger.error("íšŒì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        self.stats['total_companies'] = len(companies)
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        total_news_all = 0
        total_successful_all = 0
        
        for i in range(0, len(companies), batch_size):
            batch = companies[i:i+batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(companies) + batch_size - 1) // batch_size
            
            self.logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ íšŒì‚¬)")
            
            if parallel:
                total_news, successful = self.parallel_news_collection(batch, start_date, max_workers=2)
            else:
                total_news, successful = self.sequential_news_collection(batch, start_date)
            
            total_news_all += total_news
            total_successful_all += successful
            
            # ë°°ì¹˜ ê°„ ëŒ€ê¸° (API ì•ˆì •ì„±)
            if i + batch_size < len(companies):
                self.logger.info("â¸ï¸ ë°°ì¹˜ ê°„ ëŒ€ê¸° ì¤‘...")
                time.sleep(5)
        
        self.stats['total_news_collected'] = total_news_all
        self.stats['successful_companies'] = total_successful_all
        
        self.print_final_stats()
        return total_news_all > 0


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì „ì²´ ì¢…ëª© ë‰´ìŠ¤ ë°ì´í„° ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--quick_recovery', action='store_true', help='ë¹ ë¥¸ ë³µêµ¬ (ì£¼ìš” ì¢…ëª©ë§Œ)')
    parser.add_argument('--full_recovery', action='store_true', help='ì „ì²´ ë³µêµ¬ (ëª¨ë“  ì¢…ëª©)')
    parser.add_argument('--start_date', type=str, help='ìˆ˜ì§‘ ì‹œì‘ì¼ (YYYY-MM-DD)')
    parser.add_argument('--days', type=int, help='ìˆ˜ì§‘ ê¸°ê°„ (ì¼ìˆ˜)')
    parser.add_argument('--top_stocks', type=int, default=100, help='ë¹ ë¥¸ ë³µêµ¬ì‹œ ëŒ€ìƒ ì¢…ëª© ìˆ˜')
    parser.add_argument('--batch_size', type=int, default=50, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--parallel', action='store_true', help='ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©')
    parser.add_argument('--log_level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='ë¡œê·¸ ë ˆë²¨')
    
    args = parser.parse_args()
    
    try:
        # ë³µêµ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        recovery_manager = NewsRecoveryManager(log_level=args.log_level)
        
        # ë‚ ì§œ ì²˜ë¦¬
        start_date = None
        if args.start_date:
            start_date = args.start_date
        elif args.days:
            # daysê°€ ì§€ì •ëœ ê²½ìš° í˜„ì¬ ë‚ ì§œì—ì„œ ì—­ì‚°
            target_date = datetime.now() - timedelta(days=args.days)
            start_date = target_date.strftime('%Y-%m-%d')
        
        if args.quick_recovery:
            # ë¹ ë¥¸ ë³µêµ¬
            success = recovery_manager.quick_recovery(
                top_stocks=args.top_stocks,
                start_date=start_date,
                parallel=args.parallel
            )
        elif args.full_recovery:
            # ì „ì²´ ë³µêµ¬
            success = recovery_manager.full_recovery(
                start_date=start_date,
                parallel=args.parallel,
                batch_size=args.batch_size
            )
        else:
            # ê¸°ë³¸ê°’: ë¹ ë¥¸ ë³µêµ¬
            recovery_manager.logger.info("ê¸°ë³¸ ëª¨ë“œ: ë¹ ë¥¸ ë³µêµ¬ ì‹¤í–‰")
            success = recovery_manager.quick_recovery(
                top_stocks=args.top_stocks,
                start_date=start_date,
                parallel=False  # ê¸°ë³¸ê°’ì€ ìˆœì°¨ ì²˜ë¦¬
            )
        
        if success:
            recovery_manager.logger.info("âœ… ë‰´ìŠ¤ ë°ì´í„° ë³µêµ¬ ì„±ê³µ")
            sys.exit(0)
        else:
            recovery_manager.logger.error("âŒ ë‰´ìŠ¤ ë°ì´í„° ë³µêµ¬ ì‹¤íŒ¨")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()