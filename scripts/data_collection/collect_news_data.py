# =============================================================================
# 4. scripts/data_collection/collect_news_data.py
# =============================================================================

#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ì‹¤í–‰: python scripts/data_collection/collect_news_data.py
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import requests
import pandas as pd
import sqlite3
import time
import urllib.parse
from datetime import datetime, timedelta
import logging
from config import get_naver_news_config, get_db_connection

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class NewsCollector:
    def __init__(self):
        self.naver_config = get_naver_news_config()
        self.client_id = self.naver_config['client_id']
        self.client_secret = self.naver_config['client_secret']
        self.base_url = "https://openapi.naver.com/v1/search/news.json"
        
        if not self.client_id or not self.client_secret:
            raise ValueError("ë„¤ì´ë²„ ë‰´ìŠ¤ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    def collect_news_data(self, keyword, display=100, start=1, sort='date'):
        """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            encoded_keyword = urllib.parse.quote(keyword)
            
            params = {
                'query': encoded_keyword,
                'display': display,
                'start': start,
                'sort': sort
            }
            
            headers = {
                'X-Naver-Client-Id': self.client_id,
                'X-Naver-Client-Secret': self.client_secret
            }
            
            response = requests.get(self.base_url, params=params, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            if 'items' not in data:
                logger.warning(f"ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ: {keyword}")
                return pd.DataFrame()
            
            news_data = []
            for item in data['items']:
                news_data.append({
                    'stock_code': self._extract_stock_code(keyword),
                    'title': self._clean_html(item['title']),
                    'description': self._clean_html(item['description']),
                    'originallink': item['originallink'],
                    'link': item['link'],
                    'pubDate': self._parse_date(item['pubDate']),
                    'source': self._extract_source(item['link']),
                    'category': self._classify_news_category(item['title'], item['description']),
                    'news_type': self._classify_news_type(item['title'], item['description'])
                })
            
            return pd.DataFrame(news_data)
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ({keyword}): {e}")
            return pd.DataFrame()
    
    def _clean_html(self, text):
        """HTML íƒœê·¸ ì œê±°"""
        import re
        return re.sub('<[^<]+?>', '', text)
    
    def _parse_date(self, date_str):
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜"""
        try:
            dt = datetime.strptime(date_str, '%a, %d %b %Y %H:%M:%S %z')
            return dt.strftime('%Y-%m-%d %H:%M:%S')
        except:
            return date_str
    
    def _extract_source(self, link):
        """ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ì¶œ"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(link)
            return parsed.netloc
        except:
            return 'unknown'
    
    def _extract_stock_code(self, keyword):
        """í‚¤ì›Œë“œì—ì„œ ì¢…ëª©ì½”ë“œ ì¶”ì¶œ"""
        if keyword.isdigit() and len(keyword) == 6:
            return keyword
        return None
    
    def _classify_news_category(self, title, description):
        """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        content = f"{title} {description}".lower()
        
        fundamental_keywords = [
            'ì‹¤ì ', 'ë§¤ì¶œ', 'ì˜ì—…ìµ', 'ìˆœì´ìµ', 'ì†ìµ', 'ì¬ë¬´ì œí‘œ',
            'ë°°ë‹¹', 'ì¦ì', 'ê°ì', 'ì‚¬ì—…', 'íˆ¬ì', 'ì¸ìˆ˜', 'í•©ë³‘',
            'ê³µì‹œ', 'ì§€ë°°êµ¬ì¡°', 'ê²½ì˜ì§„', 'ì´ì‚¬íšŒ'
        ]
        
        for keyword in fundamental_keywords:
            if keyword in content:
                return 'fundamental'
        
        return 'general'
    
    def _classify_news_type(self, title, description):
        """ë‰´ìŠ¤ ìœ í˜• ë¶„ë¥˜"""
        content = f"{title} {description}".lower()
        
        if any(keyword in content for keyword in ['ì‹¤ì ', 'ë§¤ì¶œ', 'ì˜ì—…ìµ', 'ìˆœì´ìµ']):
            return 'earnings'
        
        if any(keyword in content for keyword in ['íˆ¬ì', 'í™•ì¥', 'ì§„ì¶œ', 'ì„¤ë¦½']):
            return 'expansion'
        
        if any(keyword in content for keyword in ['ë°°ë‹¹', 'ë°°ë‹¹ê¸ˆ', 'ì£¼ì£¼í™˜ì›']):
            return 'dividend'
        
        if any(keyword in content for keyword in ['CEO', 'ì‚¬ì¥', 'ê²½ì˜ì§„', 'ì´ì‚¬íšŒ']):
            return 'management'
        
        if any(keyword in content for keyword in ['ì‚°ì—…', 'ì—…ê³„', 'ì‹œì¥', 'ë™í–¥']):
            return 'industry'
        
        return 'general'
    
    def collect_stock_news(self, stock_code, company_name, days=30):
        """ê°œë³„ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            keywords = [stock_code, company_name]
            all_news = []
            
            for keyword in keywords:
                for page in range(1, 3):  # ìµœëŒ€ 2í˜ì´ì§€
                    start_idx = (page - 1) * 100 + 1
                    
                    news_data = self.collect_news_data(keyword, display=100, start=start_idx)
                    
                    if not news_data.empty:
                        news_data['stock_code'] = stock_code
                        all_news.append(news_data)
                    
                    time.sleep(0.1)
            
            if all_news:
                combined_news = pd.concat(all_news, ignore_index=True)
                combined_news = combined_news.drop_duplicates(subset=['title', 'pubDate'])
                
                # ë‚ ì§œ í•„í„°ë§
                cutoff_date = datetime.now() - timedelta(days=days)
                combined_news = combined_news[
                    pd.to_datetime(combined_news['pubDate']) > cutoff_date
                ]
                
                return combined_news
            
            return pd.DataFrame()
            
        except Exception as e:
            logger.error(f"ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ({stock_code}): {e}")
            return pd.DataFrame()
    
    def collect_all_stock_news(self, days=30, limit=None):
        """ì „ì¢…ëª© ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            with get_db_connection('stock') as conn:
                query = "SELECT stock_code, company_name FROM company_info"
                if limit:
                    query += f" LIMIT {limit}"
                
                stocks = pd.read_sql(query, conn)
            
            if stocks.empty:
                logger.error("ì¢…ëª© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¢…ëª© ê¸°ë³¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.")
                return False
            
            total_count = len(stocks)
            success_count = 0
            
            for i, row in stocks.iterrows():
                stock_code = row['stock_code']
                company_name = row['company_name']
                
                logger.info(f"ë‰´ìŠ¤ ìˆ˜ì§‘: {i+1}/{total_count} - {stock_code} ({company_name})")
                
                news_data = self.collect_stock_news(stock_code, company_name, days)
                
                if not news_data.empty:
                    with get_db_connection('news') as conn:
                        conn.execute(
                            "DELETE FROM news_articles WHERE stock_code = ?", (stock_code,)
                        )
                        news_data.to_sql('news_articles', conn, if_exists='append', index=False)
                    
                    success_count += 1
                    logger.info(f"ì €ì¥ ì™„ë£Œ: {stock_code} - {len(news_data)}ê°œ ë‰´ìŠ¤")
                
                time.sleep(0.1)
            
            logger.info(f"ì „ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{total_count}")
            return True
            
        except Exception as e:
            logger.error(f"ì „ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        collector = NewsCollector()
        
        print("ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€ í™•ì¸
        test_mode = input("í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (10ê°œ ì¢…ëª©ë§Œ ìˆ˜ì§‘) (y/N): ")
        limit = 10 if test_mode.lower() == 'y' else None
        
        # ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì •
        days = 30
        print(f"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {days}ì¼")
        
        if not test_mode.lower() == 'y':
            print("âš ï¸  ì „ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
            user_input = input("ê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if user_input.lower() != 'y':
                print("ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
                return False
        
        # ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
        success = collector.collect_all_stock_news(days, limit)
        
        if success:
            print("âœ… ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        else:
            print("âŒ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨!")
            
        return success
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    success = main()
    if not success:
        sys.exit(1)