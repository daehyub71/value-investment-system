#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìˆ˜ì •ëœ ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ - ì•„ëª¨ë ˆí¼ì‹œí”½ìš© ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
"""

import sqlite3
import requests
import re
from datetime import datetime, timedelta
from dateutil import parser as date_parser
import time
import os
from dotenv import load_dotenv

load_dotenv()

def collect_amorepacific_latest_news():
    """ì•„ëª¨ë ˆí¼ì‹œí”½ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ (2025ë…„ í¬í•¨)"""
    
    # ë„¤ì´ë²„ API ì„¤ì •
    client_id = os.getenv('NAVER_CLIENT_ID')
    client_secret = os.getenv('NAVER_CLIENT_SECRET')
    
    if not client_id or not client_secret:
        print("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    db_path = 'data/databases/news_data.db'
    company_name = 'ì•„ëª¨ë ˆí¼ì‹œí”½'
    stock_code = '090430'
    
    headers = {
        'X-Naver-Client-Id': client_id,
        'X-Naver-Client-Secret': client_secret
    }
    
    print(f"ğŸ” {company_name} ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ê¸°ì¡´ ë‰´ìŠ¤ URL ì¡°íšŒ (ì¤‘ë³µ ë°©ì§€)
    existing_urls = set()
    try:
        with sqlite3.connect(db_path) as conn:
            cursor = conn.execute("""
                SELECT DISTINCT originallink FROM news_articles 
                WHERE stock_code = ? AND originallink IS NOT NULL AND originallink != ''
            """, (stock_code,))
            existing_urls = {row[0] for row in cursor.fetchall()}
    except:
        pass
    
    print(f"ğŸ“Š ê¸°ì¡´ ë‰´ìŠ¤ URL: {len(existing_urls)}ê°œ")
    
    # ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
    all_new_news = []
    cutoff_date = datetime.now().date() - timedelta(days=30)  # ìµœê·¼ 30ì¼
    
    for page in range(1, 6):  # 5í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘
        start_index = (page - 1) * 100 + 1
        
        params = {
            'query': company_name,
            'display': 100,
            'start': start_index,
            'sort': 'date'  # ìµœì‹ ìˆœ
        }
        
        try:
            response = requests.get(
                "https://openapi.naver.com/v1/search/news.json",
                headers=headers, 
                params=params, 
                timeout=30
            )
            response.raise_for_status()
            
            data = response.json()
            news_items = data.get('items', [])
            
            if not news_items:
                break
            
            new_count = 0
            old_count = 0
            
            for item in news_items:
                # URL ì¤‘ë³µ ì²´í¬
                url = item.get('originallink', item.get('link', ''))
                if url in existing_urls:
                    continue
                
                # ë‚ ì§œ í™•ì¸
                pub_date_str = item.get('pubDate', '')
                try:
                    if pub_date_str:
                        pub_date = date_parser.parse(pub_date_str).date()
                        if pub_date >= cutoff_date:
                            all_new_news.append(item)
                            new_count += 1
                        else:
                            old_count += 1
                    else:
                        all_new_news.append(item)  # ë‚ ì§œ ì—†ìœ¼ë©´ ì¼ë‹¨ í¬í•¨
                        new_count += 1
                except:
                    all_new_news.append(item)  # íŒŒì‹± ì‹¤íŒ¨í•˜ë©´ ì¼ë‹¨ í¬í•¨
                    new_count += 1
            
            print(f"ğŸ“„ í˜ì´ì§€ {page}: ì‹ ê·œ {new_count}ê°œ, ì˜¤ë˜ëœ ë‰´ìŠ¤ {old_count}ê°œ")
            
            # ì˜¤ë˜ëœ ë‰´ìŠ¤ê°€ ë§ìœ¼ë©´ ì¤‘ë‹¨
            if old_count > new_count:
                break
                
            time.sleep(0.1)  # API ì œí•œ ì¤€ìˆ˜
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            break
    
    print(f"âœ… ì´ {len(all_new_news)}ê°œ ì‹ ê·œ ë‰´ìŠ¤ ë°œê²¬")
    
    if not all_new_news:
        print("ğŸ“° ì‹ ê·œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return True
    
    # ë‰´ìŠ¤ ì €ì¥
    saved_count = 0
    
    with sqlite3.connect(db_path) as conn:
        for item in all_new_news:
            try:
                # HTML íƒœê·¸ ì œê±°
                title = re.sub(r'<[^>]+>', '', item.get('title', ''))
                description = re.sub(r'<[^>]+>', '', item.get('description', ''))
                
                # ê°„ë‹¨í•œ ê°ì •ë¶„ì„
                content = f"{title} {description}".lower()
                positive_words = ['ì„±ì¥', 'ì¦ê°€', 'ìƒìŠ¹', 'ê°œì„ ', 'í˜¸ì¡°', 'ê¸ì •', 'ì„±ê³µ']
                negative_words = ['ê°ì†Œ', 'í•˜ë½', 'ë¶€ì§„', 'ì•…í™”', 'ìš°ë ¤', 'ë¶€ì •', 'ì‹¤íŒ¨']
                
                pos_count = sum(1 for word in positive_words if word in content)
                neg_count = sum(1 for word in negative_words if word in content)
                
                if pos_count > neg_count:
                    sentiment_score = 0.3
                    sentiment_label = 'positive'
                elif neg_count > pos_count:
                    sentiment_score = -0.3
                    sentiment_label = 'negative'
                else:
                    sentiment_score = 0.0
                    sentiment_label = 'neutral'
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
                conn.execute('''
                    INSERT OR IGNORE INTO news_articles 
                    (stock_code, title, description, originallink, link, pubDate, 
                     source, category, sentiment_score, sentiment_label, confidence_score, keywords, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    stock_code,
                    title,
                    description,
                    item.get('originallink', ''),
                    item.get('link', ''),
                    item.get('pubDate', ''),
                    'ë„¤ì´ë²„ë‰´ìŠ¤',
                    'ê¸ˆìœµ',
                    sentiment_score,
                    sentiment_label,
                    0.5,
                    f"pos:{pos_count},neg:{neg_count}",
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                ))
                
                saved_count += 1
                
            except Exception as e:
                print(f"âš ï¸ ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                continue
    
    print(f"ğŸ’¾ {saved_count}ê°œ ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
    with sqlite3.connect(db_path) as conn:
        cursor = conn.execute("""
            SELECT MIN(pubDate), MAX(pubDate), COUNT(*) 
            FROM news_articles 
            WHERE stock_code = ?
        """, (stock_code,))
        
        result = cursor.fetchone()
        if result:
            min_date, max_date, total_count = result
            print(f"ğŸ“Š ì—…ë°ì´íŠ¸ í›„ ìƒíƒœ:")
            print(f"   ê¸°ê°„: {min_date} ~ {max_date}")
            print(f"   ì´ ë‰´ìŠ¤: {total_count}ê°œ")
    
    return True

if __name__ == "__main__":
    collect_amorepacific_latest_news()
